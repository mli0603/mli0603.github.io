
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>Consistent Online Depth Network</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="icon" type="image/png" href="img/realitylabs.png"/>


    <link rel="stylesheet" href="css/app.css">
    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="js/video_comparison.js"></script>
</head>

<body>
    <div style="padding: 25px 0;">
    </div> 
</body>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                Temporally Consistent Online Depth Estimation in Dynamic Scenes
                <br>
                <small>
                    WACV 2023
                </small>
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://mli0603.github.io/">
                          Zhaoshuo Li<sup>1</sup>
                        </a>
                    </li>
                    <li>
                        <a href="https://ywwwer.github.io/">
                            Wei Ye<sup>2</sup>
                        </a>
                    </li>
                    <li>
                        <a href="https://scholar.google.com/citations?user=dmTy9EIAAAAJ&hl=en">
                            Dilin Wang<sup>2</sup>
                        </a>
                    </li>
                    <li>
                        <a href="https://www.hopkinsmedicine.org/profiles/details/francis-creighton">
                            Francis X. Creighton<sup>1</sup>
                        </a>
                    </li>
                    <li>
                        <a href="https://www.cs.jhu.edu/~rht/">
                            Russell H. Taylor<sup>1</sup>
                        </a>
                    </li>
                    <li>
                        <a href="https://www.linkedin.com/in/ganesh-venkatesh">
                            Ganesh Venkatesh<sup>2</sup>
                        </a>
                    </li>
                    <li>
                        <a href="https://mathiasunberath.github.io/">
                            Mathias Unberath<sup>1</sup>
                        </a>
                    </li>
                </ul>
                <ul class="list-inline">
                    <li>
                        <sup>1</sup> Johns Hopkins University
                    </li>
                    <li>
                        <sup>2</sup> Reality Labs, Meta Inc.
                    </li>
                </ul>
            </div>
        </div>

        <div class="row">
                <div class="col-md-4 col-md-offset-4 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="https://arxiv.org/abs/2111.09337">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://github.com/facebookresearch/CODD">
                                <h4><strong>Code</strong></h4>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>

        <div class="row">
            <div class="col-md-10 col-md-offset-1">
                <p class="text-justify">
                    We propose <b>Consistent Online Dynamic Depth (CODD)</b>, a framework for consistent depth consistency in <b>dynamic environments</b> (camera motion, object motion and object deformation) and <b>online settings</b> (no future information and real-time inference).
                    Temporally consistent depth estimation is crucial for 3D visualization in mixed reality applications.
                    Otherwise, the jitters in depth corrupt the perceptual quality significantly. 
                    However, temporal consistency is <span style="color: red">largely overlooked</span>.
                    <b>CODD</b> solves this notoriously difficult problem by integrating a novel <span style="color: rgb(44, 93, 156);">motion network</span> and <span style="color: rgb(44, 93, 156);">fusion network</span> with per-frame depth estimation solutions.
                    <b>CODD</b> improves temporal consistency in depth significantly while producing accurate estimates, for both static background and dynamic foreground objects.
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-10 col-md-offset-1">
                <h3>
                    Overview Video
                </h3>
                <div class="text-center">
                    <div style="position:relative;padding-top:56.25%;">
                        <iframe src="https://www.youtube.com/embed/aa-t7b4-iEQ" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                    </div>
                </div>
            </div>
        </div>

        <br>

        <div class="row">
            <div class="col-md-10 col-md-offset-1">
                <h3>
                    Demo
                </h3>
                <p>Move your mouse to compare the results! Switching back and forth quickly for better comparison.</p>
                <!--  video comparison code adapted from RefNerf -->
                <h4>Indoor scene</h4> 
                <div class="video-compare-container" id="indoorDiv">
                    <video class="video" id="indoor" loop playsinline autoPlay muted src="assets/demo_indoor.mov" onplay="resizeAndPlay(this)"></video>
                    <canvas height=0 class="videoMerge" id="indoorMerge"></canvas>
                </div>
                <h4>Outdoor scene</h4>
                <div class="video-compare-container" id="outdoorDiv">
                    <video class="video" id="outdoor" loop playsinline autoPlay muted src="assets/demo_outdoor.mov" onplay="resizeAndPlay(this)"></video>
                    <canvas height=0 class="videoMerge" id="outdoorMerge"></canvas>
                </div>
                <h4>Medical scene</h4>
                <div class="video-compare-container" id="medicalDiv">
                    <video class="video" id="medical" loop playsinline autoPlay muted src="assets/demo_medical.mov" onplay="resizeAndPlay(this)"></video>
                    <canvas height=0 class="videoMerge" id="medicalMerge"></canvas>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-10 col-md-offset-1">
                <h3>
                    Methodology
                </h3>
                <p>
                    <image src="img/overview.png" class="img-responsive" alt="overview"></image> 
                    <br>
                    <b>CODD</b> builds on top of per-frame stereo depth estimation network with two additions:
                    <ul>
                        <li> 
                            <span style="color: rgb(44, 93, 156);">Motion network</span> -- predicts per-pixel SE3 motion to align past estimates to the current frame,
                        </li>
                        <li> 
                            <span style="color: rgb(44, 93, 156);">Fusion network</span> -- fuses the aligned estimates for temporal consistency.
                        </li>
                    </ul>
                </p>

                <h4>Motion network</h4>
                <p>
                    The <span style="color: rgb(44, 93, 156);">motion network</span> uses depth and features from previous time point and current time point to regress per-pixel SE3 motion. 
                    The SE3 motion is regularized by a piece-wise consistent assumption to account for dynamics such as camera motion, object motion and object deformation.
                    After recovering the SE3 motion, the <span style="color: rgb(44, 93, 156);">motion network</span> aligns the observations using differentiable rendering.
                </p>
                <center>
                    <video width=80% class="large_video" controls autoplay muted loop>
                        <source src="assets/method-motion.mp4" type="video/mp4">
                        X
                    </video>
                </center>        

                <h4>Fusion network</h4>
                <p>
                    The <span style="color: rgb(44, 93, 156);">fusion network</span> regresses per-pixel weightings between the aligned observations.
                    A collection of explicit cues are extracted to guide the weighting regression, including self-similarity and cross-similarity (see paper for more details).
                    The temporally consistent depth is generated by fusing the aligned estimates.
                </p>
                <center>
                    <video width=80% class="large_video" controls autoplay muted loop>
                        <source src="assets/method-fusion.mp4" type="video/mp4">
                        X
                    </video>
                </center>        
            </div>
        </div>
            
        <div class="row">
            <div class="col-md-10 col-md-offset-1">
                <h3>
                    Citation
                </h3>
                <div class="col-md-12" style="font-size: small;">
                    @article{li2021temporally,<br>
                        &emsp;&emsp;&emsp;&emsp; title={Temporally Consistent Online Depth Estimation in Dynamic Scenes},<br>
                        &emsp;&emsp;&emsp;&emsp; author={Li, Zhaoshuo and Ye, Wei and Wang, Dilin and Creighton, Francis X and Taylor, Russell H and Venkatesh, Ganeshnbsp and Unberath, Mathias},<br>
                        &emsp;&emsp;&emsp;&emsp; journal={IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},<br>
                        &emsp;&emsp;&emsp;&emsp; year={2023}<br>
                        }
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-10 col-md-offset-1">
                <h3>
                    Poster
                </h3>
                <div class="text-center">
                    <div style="position:relative;padding-top:50%;">
                        <iframe src="assets/280-wacv-post.pdf#toolbar=0" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                    </div>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-10 col-md-offset-1">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                    We thank Johannes Kopf and Michael Zollhoefer for helpful discussions, and Vladimir Tankovich for HITNet implementation. We thank anonymous reviewers and meta-reviewers for their feedback on the paper. This work was done during an internship at Reality Labs, Meta Inc and funded in part by NIDCD K08 Grant DC019708.
                </p>
            </div>
        </div>
    </div>
</body>
</html>
