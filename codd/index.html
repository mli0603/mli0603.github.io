
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>Consistent Online Depth Network</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">


    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">
    <link rel="icon" type="image/png" href="img/realitylabs.png"/>

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
</head>

<body>
    <div style="padding: 25px 0;">
    </div> 
</body>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                Temporally Consistent Online Depth Estimation in Dynamic Scenes
                <br>
                <small>
                    WACV 2023
                </small>
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://mli0603.github.io/">
                          Zhaoshuo Li<sup>1</sup>
                        </a>
                    </li>
                    <li>
                        <a href="https://ywwwer.github.io/">
                            Wei Ye<sup>2</sup>
                        </a>
                    </li>
                    <li>
                        <a href="https://scholar.google.com/citations?user=dmTy9EIAAAAJ&hl=en">
                            Dilin Wang<sup>2</sup>
                        </a>
                    </li>
                    <li>
                        <a href="https://www.hopkinsmedicine.org/profiles/details/francis-creighton">
                            Francis X. Creighton<sup>1</sup>
                        </a>
                    </li>
                    <li>
                        <a href="https://www.cs.jhu.edu/~rht/">
                            Russell H. Taylor<sup>1</sup>
                        </a>
                    </li>
                    <li>
                        <a href="https://www.linkedin.com/in/ganesh-venkatesh">
                            Ganesh Venkatesh<sup>2</sup>
                        </a>
                    </li>
                    <li>
                        <a href="https://mathiasunberath.github.io/">
                            Mathias Unberath<sup>1</sup>
                        </a>
                    </li>
                </ul>
                <ul class="list-inline">
                    <li>
                        <sup>1</sup> Johns Hopkins University
                    </li>
                    <li>
                        <sup>2</sup> Reality Labs, Meta Inc.
                    </li>
                </ul>
            </div>
        </div>

        <div class="row">
                <div class="col-md-4 col-md-offset-4 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="https://arxiv.org/abs/2111.09337">
                            <image src="img/paper_image.png" height="60px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://github.com/facebookresearch/CODD">
                            <image src="img/github.png" height="60px">
                                <h4><strong>Code</strong></h4>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Overview Video
                </h3>
                <div class="text-center">
                    <div style="position:relative;padding-top:56.25%;">
                        <iframe src="https://www.youtube.com/embed/aa-t7b4-iEQ" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                    </div>
                </div>
            </div>
        </div>

        <br></br>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <image src="img/overview.png" class="img-responsive" alt="overview"><br>
                <p class="text-justify">
                    <b>Temporally consistent depth estimation</b> is crucial for online applications such as mixed reality. 
                    However, it is While stereo depth estimation has received substantial attention as a promising way to generate 3D information, there is relatively little work focused on maintaining temporal stability. Indeed, based on our analysis, current techniques still suffer from poor temporal consistency. Stabilizing depth temporally in dynamic scenes is challenging due to concurrent object and camera motion. In an online setting, this process is further aggravated because only past frames are available. We present a framework named <i>Consistent Online Dynamic Depth</i> (CODD) to produce temporally consistent depth estimates in dynamic scenes in an online setting. CODD augments per-frame stereo networks with novel motion and fusion networks. The motion network accounts for object and camera motion by predicting a per-pixel SE3 transformation. The fusion network improves temporal consistency in predictions by aggregating the current and previous estimates. We conduct extensive experiments across varied datasets (synthetic, outdoor, indoor and medical). We demonstrate that CODD outperforms competing methods in terms of temporal consistency and performs on par in terms of per-frame accuracy quantitatively and qualitatively.
                </p>
            </div>
        </div>
            
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
@article{li2021temporally,
title={Temporally Consistent Online Depth Estimation in Dynamic Scenes},
author={Li, Zhaoshuo and Ye, Wei and Wang, Dilin and Creighton, Francis X and Taylor, Russell H and Venkatesh, Ganesh and Unberath, Mathias},
journal={IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
year={2023}
}</textarea>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                We thank Johannes Kopf and Michael Zollhoefer for advice, and Vladimir Tankovich for HITNet implementation.
                </p>
            </div>
        </div>
    </div>
</body>
</html>
